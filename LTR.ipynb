{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Learning to Rank (LTR)\n",
    "\n",
    "\n",
    "### Include required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "def print_message(s):\n",
    "    print(\"[{}] {}\".format(datetime.datetime.utcnow().strftime(\"%b %d, %H:%M:%S\"), s), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test data readers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line):\n",
    "        tokens                              = line.strip().split(' ')\n",
    "        qid                                 = -1\n",
    "        feat                                = []\n",
    "        label                               = int(tokens[0])\n",
    "        for i in range(FEAT_COUNT):\n",
    "            feat.append(0)\n",
    "        for i in range(1, len(tokens)):\n",
    "            sub_tokens                      = tokens[i].split(':')\n",
    "            if sub_tokens[0] == 'qid':\n",
    "                qid                         = int(sub_tokens[1])\n",
    "            else:\n",
    "                feat_idx                    = int(sub_tokens[0])\n",
    "                feat_val                    = float(sub_tokens[1])\n",
    "                feat[feat_idx - 1]          = int(feat_val * FEAT_SCALE)\n",
    "        return qid, label, feat\n",
    "    \n",
    "    \n",
    "class DataReaderTrain():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "        self.__load_data(self.data_file)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "\n",
    "    def __load_data(self, data_file):\n",
    "        self.data                           = {}\n",
    "        with open(data_file, mode='r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                qid, label, feat            = DataUtils.parse_line(line)\n",
    "                if qid not in self.data:\n",
    "                    self.data[qid]          = {}\n",
    "                if label not in self.data[qid]:\n",
    "                    self.data[qid][label]   = []\n",
    "                self.data[qid][label].append(feat)\n",
    "        self.data                           = {k: v for k, v in self.data.items() if len(v) > 1}\n",
    "        self.qids                           = list(self.data.keys())\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = [np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32) for i in range(2)]\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        for i in range(2):\n",
    "            self.features[i].fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = random.sample(self.qids, MB_SIZE)\n",
    "        for i in range(MB_SIZE):\n",
    "            labels                          = random.sample(self.data[qids[i]].keys(), 2)\n",
    "            labels.sort(reverse=True)\n",
    "            for j in range(2):\n",
    "                feats                       = self.data[qids[i]][labels[j]]\n",
    "                feat                        = feats[random.randint(0, len(feats) - 1)]\n",
    "                for k in range(FEAT_COUNT):\n",
    "                    self.features[j][i, k]  = feat[k] / FEAT_SCALE\n",
    "        return [torch.from_numpy(self.features[i]) for i in range(2)], torch.from_numpy(self.labels)                            # if you want to run on CPU\n",
    "        #return [torch.from_numpy(self.features[i]).to(DEVICE) for i in range(2)], torch.from_numpy(self.labels).to(DEVICE)      # if you want to run on GPU\n",
    "    \n",
    "    \n",
    "class DataReaderTest():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reader                         = open(self.data_file, mode='r', encoding=\"utf-8\")\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32)\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        self.features.fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = []\n",
    "        labels                              = []\n",
    "        cnt                                 = 0\n",
    "        for i in range(MB_SIZE):\n",
    "            line                            = self.reader.readline()\n",
    "            if line == '':\n",
    "                raise StopIteration\n",
    "                break\n",
    "            qid, label, feat                = DataUtils.parse_line(line)\n",
    "            qids.append(qid)\n",
    "            labels.append(label)\n",
    "            for j in range(FEAT_COUNT):\n",
    "                self.features[i, j]         = feat[j] / FEAT_SCALE\n",
    "            cnt                            += 1\n",
    "        return torch.from_numpy(self.features), qids, labels, cnt               # if you want to run on CPU\n",
    "        #return torch.from_numpy(self.features).to(DEVICE), qids, labels, cnt    # if you want to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        layers          = []\n",
    "        last_dim        = FEAT_COUNT\n",
    "        for i in range(NUM_HIDDEN_LAYERS):\n",
    "            layers.append(nn.Linear(last_dim, NUM_HIDDEN_NODES))\n",
    "            layers.append(nn.Tanh())\n",
    "            last_dim    = NUM_HIDDEN_NODES\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) * SCALE\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths and readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 16, 08:17:17] Starting\n",
      "[Jan 16, 08:19:05] Data loaded\n"
     ]
    }
   ],
   "source": [
    "print_message('Starting')\n",
    "DATA_DIR                    = 'data/'\n",
    "DATA_FILE_TRAIN             = os.path.join(DATA_DIR, 'train.txt')\n",
    "DATA_FILE_TEST              = os.path.join(DATA_DIR, 'test.txt')\n",
    "MODEL_FILE                  = os.path.join(DATA_DIR, \"ltr.{}.dnn\")\n",
    "FEAT_COUNT                  = 136\n",
    "FEAT_SCALE                  = 1000\n",
    "MB_SIZE                     = 1024\n",
    "READER_TRAIN                = DataReaderTrain(DATA_FILE_TRAIN)\n",
    "READER_TRAIN_ITER           = iter(READER_TRAIN)\n",
    "print_message('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 16, 08:19:05] Number of learnable parameters: 34177\n",
      "[Jan 16, 08:19:05] Learning rate: 0.0001\n",
      "[Jan 16, 08:46:05] epoch:1, loss: 0.6607856009795796, ndcg: 48.360931101927584\n",
      "[Jan 16, 09:15:05] epoch:2, loss: 0.6386688793791109, ndcg: 48.805701347345504\n",
      "[Jan 16, 09:38:29] epoch:3, loss: 0.6285003042430617, ndcg: 49.47443496100645\n",
      "[Jan 16, 10:03:08] epoch:4, loss: 0.6221757594539667, ndcg: 49.684954978203486\n",
      "[Jan 16, 10:31:45] epoch:5, loss: 0.616008175900788, ndcg: 49.27310921665338\n",
      "[Jan 16, 10:54:53] epoch:6, loss: 0.6110261111753061, ndcg: 50.353521573243476\n",
      "[Jan 16, 11:30:00] epoch:7, loss: 0.6053921825441648, ndcg: 50.5759519077917\n",
      "[Jan 16, 12:21:25] epoch:8, loss: 0.6015463659205125, ndcg: 50.553946550455784\n",
      "[Jan 16, 12:21:25] Finished training\n"
     ]
    }
   ],
   "source": [
    "DEVICE                      = torch.device(\"cpu\")       # if you want to run on CPU\n",
    "#DEVICE                      = torch.device(\"cuda:0\")    # if you want to run on GPU\n",
    "NUM_HIDDEN_NODES            = 128\n",
    "NUM_HIDDEN_LAYERS           = 2\n",
    "EPOCH_SIZE                  = 8192\n",
    "NUM_EPOCHS                  = 8\n",
    "LEARNING_RATE               = 0.0001\n",
    "SCALE                       = torch.tensor([10], dtype=torch.float).to(DEVICE)\n",
    "EVAL_PER_EPOCH              = True\n",
    "\n",
    "torch.manual_seed(1)\n",
    "net                         = DNN()                 # if you want to run on CPU\n",
    "#net                         = DNN().to(DEVICE)      # if you want to run on GPU\n",
    "criterion                   = nn.CrossEntropyLoss()\n",
    "optimizer                   = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print_message('Number of learnable parameters: {}'.format(net.parameter_count()))\n",
    "print_message('Learning rate: {}'.format(LEARNING_RATE))\n",
    "for ep_idx in range(NUM_EPOCHS):\n",
    "    train_loss              = 0.0\n",
    "    net.train()\n",
    "    for mb_idx in range(EPOCH_SIZE):\n",
    "        features, labels    = next(READER_TRAIN_ITER)\n",
    "        out                 = torch.cat(tuple([net(features[i]) for i in range(2)]), 1)\n",
    "        loss                = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss         += loss.item()\n",
    "    #torch.save(net, MODEL_FILE.format(ep_idx + 1))\n",
    "    if EVAL_PER_EPOCH:\n",
    "        net.eval()\n",
    "        reader_test         = DataReaderTest(DATA_FILE_TEST)\n",
    "        reader_test_iter    = iter(reader_test)\n",
    "        results             = {}\n",
    "        for features, qids, labels, cnt in reader_test_iter:\n",
    "            out             = net(features)\n",
    "            #out             = out.data.cpu()      # if you want to run on GPU\n",
    "            row_cnt         = len(qids)\n",
    "            for i in range(row_cnt):\n",
    "                if qids[i] not in results:\n",
    "                    results[qids[i]] = []\n",
    "                results[qids[i]].append((labels[i], out[i][0]))\n",
    "        ndcg                = 0\n",
    "        for qid, docs in results.items():\n",
    "            ranked          = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "            for i in range(len(ranked)):\n",
    "                rank        = i + 1\n",
    "                label       = ranked[i][0]\n",
    "                ndcg       += ((2**label - 1) / math.log2(rank + 1))\n",
    "        ndcg               /= len(qids)\n",
    "        print_message('epoch:{}, loss: {}, ndcg: {}'.format(ep_idx + 1, train_loss / EPOCH_SIZE, ndcg))\n",
    "    else:\n",
    "        print_message('epoch:{}, loss: {}'.format(ep_idx + 1, train_loss / EPOCH_SIZE))\n",
    "print_message('Finished training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
