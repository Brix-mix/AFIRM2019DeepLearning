{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duet on PyTorch\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1610.08136.pdf\n",
    "\n",
    "\n",
    "### Include required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "def print_message(s):\n",
    "    print(\"[{}] {}\".format(datetime.datetime.utcnow().strftime(\"%b %d, %H:%M:%S\"), s), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "\n",
    "    def __init__(self, data_file, num_meta_cols, multi_pass):\n",
    "        self.num_meta_cols                  = num_meta_cols\n",
    "        self.multi_pass                     = multi_pass\n",
    "        self.regex_drop_char                = re.compile('[^a-z0-9\\s]+')\n",
    "        self.regex_multi_space              = re.compile('\\s+')\n",
    "        self.__load_ngraphs()\n",
    "        self.__load_idfs()\n",
    "        self.__init_data(data_file)\n",
    "        self.__allocate_minibatch()\n",
    "\n",
    "    def __tokenize(self, s, max_terms):\n",
    "        return self.regex_multi_space.sub(' ', self.regex_drop_char.sub(' ', s.lower())).strip().split()[:max_terms]\n",
    "\n",
    "    def __load_ngraphs(self):\n",
    "        global NUM_NGRAPHS\n",
    "        self.ngraphs                        = {}\n",
    "        self.max_ngraph_len                 = 0\n",
    "        with open(DATA_FILE_NGRAPHS, mode='r', encoding=\"utf-8\") as f:\n",
    "            reader                          = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                self.ngraphs[row[0]]        = int(row[1]) - 1\n",
    "                self.max_ngraph_len         = max(self.max_ngraph_len, len(row[0]))\n",
    "        NUM_NGRAPHS                         = len(self.ngraphs)\n",
    "        \n",
    "    def __load_idfs(self):\n",
    "        self.idfs                           = {}\n",
    "        with open(DATA_FILE_IDFS, mode='r', encoding=\"utf-8\") as f:\n",
    "            reader                          = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                self.idfs[row[0]]           = float(row[1])\n",
    "\n",
    "    def __init_data(self, file_name):\n",
    "        self.reader                         = open(file_name, mode='r', encoding=\"utf-8\")\n",
    "        self.num_docs                       = len(self.reader.readline().split('\\t')) - self.num_meta_cols - 1\n",
    "        self.reader.seek(0)\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = {}\n",
    "        if ARCH_TYPE != 1:\n",
    "            self.features['local']          = [] \n",
    "        if ARCH_TYPE > 0:\n",
    "            self.features['dist_q']         = np.zeros((MB_SIZE, NUM_NGRAPHS, MAX_QUERY_TERMS), dtype=np.float32)\n",
    "            self.features['dist_d']         = []\n",
    "        for i in range(self.num_docs):\n",
    "            if ARCH_TYPE != 1:\n",
    "                self.features['local'].append(np.zeros((MB_SIZE, MAX_DOC_TERMS, MAX_QUERY_TERMS), dtype=np.float32))\n",
    "            if ARCH_TYPE > 0:\n",
    "                self.features['dist_d'].append(np.zeros((MB_SIZE, NUM_NGRAPHS, MAX_DOC_TERMS), dtype=np.float32))\n",
    "        self.features['labels']             = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        self.features['meta']               = []\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        if ARCH_TYPE > 0:\n",
    "            self.features['dist_q'].fill(np.float32(0))\n",
    "        for i in range(self.num_docs):\n",
    "            if ARCH_TYPE != 1:\n",
    "                self.features['local'][i].fill(np.float32(0))\n",
    "            if ARCH_TYPE > 0:\n",
    "                self.features['dist_d'][i].fill(np.float32(0))\n",
    "        self.features['meta'].clear()\n",
    "\n",
    "    def get_minibatch(self):\n",
    "        self.__clear_minibatch()\n",
    "        for i in range(MB_SIZE):\n",
    "            row                             = self.reader.readline()\n",
    "            if row == '':\n",
    "                if self.multi_pass:\n",
    "                    self.reader.seek(0)\n",
    "                    row                     = self.reader.readline()\n",
    "                else:\n",
    "                    break\n",
    "            cols                            = row.split('\\t')\n",
    "            q                               = self.__tokenize(cols[self.num_meta_cols], MAX_QUERY_TERMS)\n",
    "            ds                              = [self.__tokenize(cols[self.num_meta_cols + i + 1], MAX_DOC_TERMS) for i in range(self.num_docs)]\n",
    "            if ARCH_TYPE != 1:\n",
    "                for d in range(self.num_docs):\n",
    "                    for j in range(len(ds[d])):\n",
    "                        for k in range(len(q)):\n",
    "                            if ds[d][j] == q[k]:\n",
    "                                self.features['local'][d][i, j, k] = self.idfs[q[k]]\n",
    "            if ARCH_TYPE > 0:\n",
    "                for j in range(self.num_docs + 1):\n",
    "                    terms = q if j == 0 else ds[j - 1]\n",
    "                    for t in range(len(terms)):\n",
    "                        term = '#' + terms[t] + '#'\n",
    "                        term_len = len(term)\n",
    "                        for k in range(term_len):\n",
    "                            for l in range(1, self.max_ngraph_len + 1):\n",
    "                                if k + l < term_len:\n",
    "                                    ngraph_idx = self.ngraphs.get(term[k : k + l])\n",
    "                                    if ngraph_idx != None:\n",
    "                                        if j == 0:\n",
    "                                            self.features['dist_q'][i, ngraph_idx, t] += 1\n",
    "                                        else:\n",
    "                                            self.features['dist_d'][j - 1][i, ngraph_idx, t] += 1\n",
    "            self.features['meta'].append(tuple(cols[:self.num_meta_cols]))\n",
    "        return self.features\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reader.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define duet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class Duet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Duet, self).__init__()\n",
    "        self.duet_local             = nn.Sequential(nn.Conv1d(MAX_DOC_TERMS, NUM_HIDDEN_NODES, kernel_size=1),\n",
    "                                        nn.Tanh(),\n",
    "                                        Flatten(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES * MAX_QUERY_TERMS, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE))\n",
    "        self.duet_dist_q            = nn.Sequential(nn.Conv1d(NUM_NGRAPHS, NUM_HIDDEN_NODES, kernel_size=3),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.MaxPool1d(POOLING_KERNEL_WIDTH_QUERY),\n",
    "                                        Flatten(),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh())\n",
    "        self.duet_dist_d            = nn.Sequential(nn.Conv1d(NUM_NGRAPHS, NUM_HIDDEN_NODES, kernel_size=3),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.MaxPool1d(POOLING_KERNEL_WIDTH_DOC, stride=1),\n",
    "                                        nn.Conv1d(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES, kernel_size=1),\n",
    "                                        nn.Tanh())\n",
    "        self.duet_dist              = nn.Sequential(Flatten(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES * NUM_POOLING_WINDOWS_DOC, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE))\n",
    "        self.duet_comb              = nn.Sequential(nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES, NUM_HIDDEN_NODES),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Dropout(p=DROPOUT_RATE),\n",
    "                                        nn.Linear(NUM_HIDDEN_NODES, 1),\n",
    "                                        nn.Tanh())\n",
    "\n",
    "    def forward(self, x_local, x_dist_q, x_dist_d):\n",
    "        if ARCH_TYPE != 1:\n",
    "            h_local                 = self.duet_local(x_local)\n",
    "        if ARCH_TYPE > 0:\n",
    "            h_dist_q                = self.duet_dist_q(x_dist_q)\n",
    "            h_dist_d                = self.duet_dist_d(x_dist_d)\n",
    "            h_dist                  = self.duet_dist(h_dist_q.unsqueeze(-1)*h_dist_d)\n",
    "        y_score                     = self.duet_comb((h_local + h_dist) if ARCH_TYPE == 2 else (h_dist if ARCH_TYPE == 1 else h_local))\n",
    "        return y_score\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 16, 07:49:08] Starting\n",
      "[Jan 16, 07:49:10] Number of learnable parameters: 39020545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Duet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Flatten. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 16, 09:22:11] epoch:1, loss: 0.44426760892383754, mrr: 0.15829918078108945\n",
      "[Jan 16, 10:55:16] epoch:2, loss: 0.35457725677406415, mrr: 0.24923528292461364\n",
      "[Jan 16, 12:28:01] epoch:3, loss: 0.32025368127506226, mrr: 0.21839168524689373\n",
      "[Jan 16, 14:00:22] epoch:4, loss: 0.30747560365125537, mrr: 0.21919354728482243\n",
      "[Jan 16, 14:00:22] Finished\n"
     ]
    }
   ],
   "source": [
    "DEVICE                          = torch.device(\"cuda:0\")    # torch.device(\"cpu\"), if you want to run on CPU instead\n",
    "ARCH_TYPE                       = 2\n",
    "MAX_QUERY_TERMS                 = 20\n",
    "MAX_DOC_TERMS                   = 200\n",
    "NUM_HIDDEN_NODES                = 512\n",
    "TERM_WINDOW_SIZE                = 3\n",
    "POOLING_KERNEL_WIDTH_QUERY      = MAX_QUERY_TERMS - TERM_WINDOW_SIZE + 1 # 20 - 3 + 1 = 18\n",
    "POOLING_KERNEL_WIDTH_DOC        = 100\n",
    "NUM_POOLING_WINDOWS_DOC         = (MAX_DOC_TERMS - TERM_WINDOW_SIZE + 1) - POOLING_KERNEL_WIDTH_DOC + 1 # (200 - 3 + 1) - 100 + 1 = 99\n",
    "NUM_NGRAPHS                     = 0\n",
    "DROPOUT_RATE                    = 0.5\n",
    "MB_SIZE                         = 1024\n",
    "EPOCH_SIZE                      = 512\n",
    "NUM_EPOCHS                      = 4\n",
    "LEARNING_RATE                   = 1e-3\n",
    "DATA_DIR                        = 'data/'\n",
    "DATA_FILE_NGRAPHS               = os.path.join(DATA_DIR, \"ngraphs.txt\")\n",
    "DATA_FILE_IDFS                  = os.path.join(DATA_DIR, \"idf.norm.tsv\")\n",
    "DATA_FILE_TRAIN                 = os.path.join(DATA_DIR, \"triples.train.full.tsv\")\n",
    "DATA_FILE_DEV                   = os.path.join(DATA_DIR, \"top1000.dev.tsv\")\n",
    "QRELS_DEV                       = os.path.join(DATA_DIR, \"qrels.dev.tsv\")\n",
    "MODEL_FILE                      = os.path.join(DATA_DIR, \"duet.{}.dnn\")\n",
    "\n",
    "READER_TRAIN                    = DataReader(DATA_FILE_TRAIN, 0, True)\n",
    "READER_DEV                      = DataReader(DATA_FILE_DEV, 2, False)\n",
    "\n",
    "qrels                           = {}\n",
    "with open(QRELS_DEV, mode='r', encoding=\"utf-8\") as f:\n",
    "    reader                      = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        qid                     = int(row[0])\n",
    "        did                     = int(row[2])\n",
    "        if qid not in qrels:\n",
    "            qrels[qid]          = []\n",
    "        qrels[qid].append(did)\n",
    "\n",
    "scores                          = {}\n",
    "for qid in qrels.keys():\n",
    "    scores[qid]                 = {}\n",
    "\n",
    "torch.manual_seed(1)\n",
    "print_message('Starting')\n",
    "net                     = Duet()\n",
    "net                     = net.to(DEVICE)\n",
    "criterion               = nn.CrossEntropyLoss()\n",
    "optimizer               = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print_message('Number of learnable parameters: {}'.format(net.parameter_count()))\n",
    "for ep_idx in range(NUM_EPOCHS):\n",
    "    train_loss          = 0.0\n",
    "    for docs in scores.values():\n",
    "        docs.clear()\n",
    "    net.train()\n",
    "    for mb_idx in range(EPOCH_SIZE):\n",
    "        features        = READER_TRAIN.get_minibatch()\n",
    "        if ARCH_TYPE == 0:\n",
    "            out         = torch.cat(tuple([net(torch.from_numpy(features['local'][i]).to(DEVICE), None, None) for i in range(READER_TRAIN.num_docs)]), 1)\n",
    "        elif ARCH_TYPE == 1:\n",
    "            out         = torch.cat(tuple([net(None, torch.from_numpy(features['dist_q']).to(DEVICE), torch.from_numpy(features['dist_d'][i]).to(DEVICE)) for i in range(READER_TRAIN.num_docs)]), 1)\n",
    "        else:\n",
    "            out         = torch.cat(tuple([net(torch.from_numpy(features['local'][i]).to(DEVICE), torch.from_numpy(features['dist_q']).to(DEVICE), torch.from_numpy(features['dist_d'][i]).to(DEVICE)) for i in range(READER_TRAIN.num_docs)]), 1)\n",
    "        loss            = criterion(out, torch.from_numpy(features['labels']).to(DEVICE))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss     += loss.item()\n",
    "    torch.save(net, MODEL_FILE.format(ep_idx + 1))\n",
    "    is_complete         = False\n",
    "    READER_DEV.reset()\n",
    "    net.eval()\n",
    "    while not is_complete:\n",
    "        features        = READER_DEV.get_minibatch()\n",
    "        if ARCH_TYPE == 0:\n",
    "            out         = net(torch.from_numpy(features['local'][0]).to(DEVICE), None, None)\n",
    "        elif ARCH_TYPE == 1:\n",
    "            out         = net(None, torch.from_numpy(features['dist_q']).to(DEVICE), torch.from_numpy(features['dist_d'][0]).to(DEVICE))\n",
    "        else:\n",
    "            out         = net(torch.from_numpy(features['local'][0]).to(DEVICE), torch.from_numpy(features['dist_q']).to(DEVICE), torch.from_numpy(features['dist_d'][0]).to(DEVICE))\n",
    "        meta_cnt        = len(features['meta'])\n",
    "        out             = out.data.cpu()\n",
    "        for i in range(meta_cnt):\n",
    "            q           = int(features['meta'][i][0])\n",
    "            d           = int(features['meta'][i][1])\n",
    "            scores[q][d]= out[i][0]\n",
    "        is_complete     = (meta_cnt < MB_SIZE)\n",
    "    mrr                 = 0\n",
    "    for qid, docs in scores.items():\n",
    "        ranked          = sorted(docs, key=docs.get, reverse=True)\n",
    "        for i in range(len(ranked)):\n",
    "            if ranked[i] in qrels[qid]:\n",
    "                mrr    += 1 / (i + 1)\n",
    "                break\n",
    "    mrr                /= len(qrels)\n",
    "    print_message('epoch:{}, loss: {}, mrr: {}'.format(ep_idx + 1, train_loss / EPOCH_SIZE, mrr))\n",
    "print_message('Finished')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
